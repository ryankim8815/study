# k-최근접 이웃 회귀

## 학습 목표: 지도 학습의 한 종류인 회귀 문제를 이해하고 k-최근접 이웃 알고리즘을 사용할 수 있다.

### 1. k-최근접 이웃 회귀

- 설명
  - 지도학습의 분류
    - 분류:
      - 샘플을 몇 개의 클래스 중 하나로 분류하는 문제
      - 예시) 본 교재의 2장
    - 회귀(Regression):
      - 정해진 클래스 없이 임의의 숫자를 예측하는 문제
      - 두 변수 사이의 상관관계를 분석하는 방법
      - 예시) 내년도 경제 성장률 예측, 배달 도착 시간 예측
      - k-최근접 이웃 분류 VS k-최근접 이웃 회귀
        - k-최근접 이웃 분류:
          1. 예측하려는 샘플에 가장 가까운 샘플 k개 선택
          2. 해당 샘플들의 클래스 확인
          3. 다수의 클래스를 새로운 클래스로 예측
        - k-최근접 이웃 회귀:
          1. 예측하려는 샘플에 가장 가까운 샘플 k개 선택(동일)
          2. 이웃한 샘플의 타깃은 임의의 수치임(클래스가 아님)
          3. 샘플 타깃 값들의 수치들 평균으로 예측값을 구함

### 2. 데이터 준비

- 설명
  - 농어 데이터 - 길이로 무게 예측
    - 특성: 길이
    - 타깃: 무게

### 3. R\*\*2

- 설명
  - KNeighborsRegressor: 사이킷런에서 k-최근접 이웃 회귀 알고리즘을 구현한 클래스
  - KNeighborsRegressor.score(input, target):
    - 분류: 샘플을 정확하게 분류한 개수 비율
    - 회귀: 결정계수(Coefficient of determination, R\*\*2)
      - 계산 방법: `R**2 = 1 - {(타깃 - 예측)**2 의 합} / {(타깃 - 평균)**2 의 합}`
      - R\*\*2가 0에 가까운 경우: 타깃의 평균 정보를 예측하는 수준
      - R\*\*2가 1에 가까운 경우: 예측이 타깃에 가까운 경우

### 4. 과대적합 VS 과소적합

- 설명
  - 과대적합(Overfitting): 훈련 세트에서는 점수가 좋지만, 테스트에서 점수가 나쁜 경우
  - 과소적합(Underfitting): 훈련 세트보다 테스트 점수가 높거나, 두 점수가 모두 낮은 경우
